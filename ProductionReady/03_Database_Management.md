# Technical Guide: Database Management for Production

This project currently uses an SQLite database stored at `instance/creditrobot.db`. While SQLite is simple for development, managing it in a production Docker environment requires careful consideration for data persistence and scalability. This guide covers strategies for handling the SQLite database and discusses more robust alternatives.

## 1. Understanding the Challenge with SQLite and Docker

Docker containers are, by default, stateless. If the application writes data (like an SQLite database file) inside the container's filesystem, this data will be lost when the container is removed or replaced. The `instance/creditrobot.db` file needs to persist independently of the container's lifecycle.

The `Dockerfile` (from Guide `01`) copies the entire application directory, including the `instance` folder if it's not in `.dockerignore`. This means a *copy* of the database at build time is included in the image. This is generally **not** what you want for production data, as any changes made by the running application will only affect the container's copy, not the image's copy, and will be lost if the container is destroyed.

## 2. Option A: Persisting SQLite with Docker Volumes (Recommended for SQLite)

Docker volumes are the preferred way to persist data generated by and used by Docker containers. Volumes are managed by Docker and exist outside the container's writable layer.

### 2.1. How to Use a Docker Volume

1.  **Ensure `instance/` is NOT in `.dockerignore` (for initial image structure):**
    If you want the `instance` directory structure to be present in the image (even if the DB file itself will be managed by a volume), ensure `instance/` is *not* in your `.dockerignore`. If it *is* in `.dockerignore`, you might need an entrypoint script to create `instance/` if your app expects it to exist.
    However, it's often better to ensure your application creates the `instance` directory if it doesn't exist, e.g., in `app.py` before initializing SQLAlchemy:
    ```python
    # app.py
    import os
    # ... other imports ...

    # Determine the absolute path for the instance folder
    instance_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'instance')
    if not os.path.exists(instance_path):
        os.makedirs(instance_path)

    app = Flask(__name__, instance_path=instance_path) # Or Flask(__name__, instance_relative_config=True) and set app.config['SQLALCHEMY_DATABASE_URI']
    app.config['SQLALCHEMY_DATABASE_URI'] = f"sqlite:///{os.path.join(instance_path, 'creditrobot.db')}"
    # ... rest of your app setup ...
    ```
    This makes your app more robust to missing directories.

2.  **Run the container with a volume mount:**
    You can use either a named volume or a bind mount.

    *   **Named Volume (Preferred for Docker managing the storage):**
        Docker creates and manages the volume.
        ```bash
        # Create a named volume (optional, Docker creates it on first use if not present)
        docker volume create creditrobot-data

        # Run the container, mounting the named volume to /app/instance
        docker run -d -p 5000:5000                -v creditrobot-data:/app/instance                --name creditrobot_app                creditrobot-app
        ```
        In this command:
        *   `-v creditrobot-data:/app/instance`: Mounts the named volume `creditrobot-data` to the `/app/instance` directory inside the container. The `creditrobot.db` file will now be stored within this volume.

    *   **Bind Mount (Mounts a host directory):**
        You specify a path on the host machine.
        ```bash
        # Create a directory on your host machine if it doesn't exist
        mkdir -p /path/on/your/host/creditrobot_data

        # Run the container, mounting the host directory
        docker run -d -p 5000:5000                -v /path/on/your/host/creditrobot_data:/app/instance                --name creditrobot_app                creditrobot-app
        ```
        Replace `/path/on/your/host/creditrobot_data` with the actual absolute path on your Docker host. This directory will then contain `creditrobot.db`.

### 2.2. Using Docker Compose for Volume Management

If you're using Docker Compose (highly recommended for managing services), define the volume in your `docker-compose.yml`:

```yaml
version: '3.8'

services:
  web:
    build: .
    image: creditrobot-app # Optional: specify image name
    ports:
      - "5000:5000"
    volumes:
      - creditrobot_db_data:/app/instance # Mounts the named volume
    # environment:
    #   - FLASK_APP=app.py
    #   - FLASK_ENV=production
    #   - GUNICORN_LOG_LEVEL=info

volumes:
  creditrobot_db_data: # Defines the named volume
```
Then run `docker-compose up -d`. Docker Compose will automatically create and manage the `creditrobot_db_data` volume. The data will persist across container restarts and updates.

### 2.3. Database Migrations (Alembic) with Docker Volumes

Your project uses Alembic for migrations. When using volumes:
*   The database file in the volume will be the source of truth.
*   Run migrations against this persisted database. You can do this by `exec`-ing into the running container:
    ```bash
    docker exec -it <container_name_or_id> flask db upgrade
    ```
    Or, if using Docker Compose:
    ```bash
    docker-compose exec web flask db upgrade
    ```
    It's often good practice to run migrations as a separate step *before* starting the new version of the application, especially if there are schema changes that are not backward compatible. Some entrypoint scripts for Docker images include logic to automatically run migrations on startup.

## 3. Option B & C: Moving to External Databases (PostgreSQL/MySQL)

While SQLite with volumes can work for smaller applications, it has limitations for production use, especially concerning concurrent writes, scalability, and robust backup/restore features. For more demanding applications, consider moving to a dedicated relational database server like PostgreSQL or MySQL.

This can be achieved by:
*   **Running another Docker container:** Use official Docker images for PostgreSQL or MySQL and link your application container to it (e.g., via Docker Compose).
*   **Using a managed database service:** Cloud providers (AWS RDS, Google Cloud SQL, Azure Database) offer managed database services that handle setup, maintenance, backups, and scaling.

### 3.1. Changes Required in the Application

If you switch to PostgreSQL or MySQL:

1.  **Install appropriate Python driver:**
    *   For PostgreSQL: `psycopg2-binary` (add to `requirements.txt`)
    *   For MySQL: `mysqlclient` or `PyMySQL` (add to `requirements.txt`)

2.  **Update SQLAlchemy Database URI:**
    In `app.py` (or your configuration management system), change `SQLALCHEMY_DATABASE_URI`. This should be read from an environment variable for security and flexibility.

    Example for PostgreSQL:
    ```python
    # app.py
    import os
    # ...
    # Default to SQLite for local dev if DB_URL is not set
    default_db_uri = f"sqlite:///{os.path.join(app.instance_path, 'creditrobot.db')}"
    app.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get('DATABASE_URL', default_db_uri)
    ```
    The `DATABASE_URL` environment variable would look like:
    `postgresql://user:password@host:port/dbname`

3.  **Database Migrations:** Alembic will still work, but the connection string used by Alembic (in `env.py` or configured via `SQLALCHEMY_DATABASE_URI`) needs to point to the new database. You'll need to initialize a new set of migrations or adapt existing ones if you are migrating data.

### 3.2. Example Docker Compose with PostgreSQL:

```yaml
version: '3.8'

services:
  web:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://user:password@db:5432/creditrobot_prod
      # - FLASK_APP=app.py
      # - FLASK_ENV=production
    depends_on:
      - db
    volumes: # If you still have other things in /app/instance you need to persist
      - other_data:/app/instance

  db:
    image: postgres:15-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=creditrobot_prod
    ports: # Optional: expose PostgreSQL port to host for direct access/debugging
      - "5432:5432"

volumes:
  postgres_data:
  other_data: # If you had other data in instance folder
```
This setup creates two services: `web` (your Flask app) and `db` (PostgreSQL). The `web` service connects to `db` using the hostname `db`.

## 4. Backup Strategy for SQLite with Volumes

If you stick with SQLite and Docker volumes:
*   **Manual Backup:** You can `docker exec` into the container and use the `sqlite3` command-line tool to back up the database to a `.sql` file or copy the `.db` file from the volume.
    ```bash
    # To copy the .db file directly from where Docker stores the volume (more complex to locate)
    # Or, if using a bind mount, simply copy the file from the host path.
    # Easier: copy from within a temporary container that mounts the same volume:
    docker run --rm -v creditrobot-data:/db_backup_source -v $(pwd)/backups:/db_backup_target alpine            cp /db_backup_source/creditrobot.db /db_backup_target/creditrobot_backup_$(date +%Y%m%d%H%M%S).db
    ```
    (Ensure `$(pwd)/backups` exists or adjust the target path).
*   **Automated Backup:** Use a cron job on the Docker host or a dedicated backup Docker container (like [tiredofit/docker-db-backup](https://hub.docker.com/r/tiredofit/docker-db-backup) which supports SQLite) that mounts the volume and performs regular backups to another location (e.g., cloud storage).

## Conclusion

For the CreditRobot application using SQLite, **Docker volumes are essential for data persistence**. Set this up carefully. For future growth or more demanding requirements, transitioning to a dedicated database server like PostgreSQL or MySQL (containerized or managed) is the standard production approach.
